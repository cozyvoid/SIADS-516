{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This homework assignment uses the Yelp Academic dataset, with which you should now be familiar.\n",
    "We have created a few cells to get you started, but you're largely on your own to devise solutions to the\n",
    "\"real-world\" questions below.\n",
    "\n",
    "In this assignment, provide solutions that use spark.sql() calls to query the dataset. For example, to find the answer to \"How many users have more than 100 \"cool\" votes?\", this:\n",
    "```\n",
    "query = \"\"\"\n",
    "SELECT count(*) FROM user WHERE cool > 100\n",
    "\"\"\"\n",
    "spark.sql(query).show()\n",
    "```\n",
    "is similar to:\n",
    "```\n",
    "user.filter('cool > 100').show()\n",
    "```\n",
    "But in this assignment, use the first approach. The autograder will check for the use of `spark.sql()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our usual Spark mantra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"local[*]\")\n",
    "    .appName(\"My First Spark application\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc._conf.set(\"spark.default.parallelism\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output\n",
    "```\n",
    "Setting default log level to \"WARN\".\n",
    "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
    "24/12/17 01:45:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "<pyspark.conf.SparkConf at 0x7f5310332ef0>\n",
    "``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the JSON files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business = spark.read.json(\n",
    "    \"../../assets/data/yelp_academic/yelp_academic_dataset_business.json.gz\"\n",
    ")\n",
    "checkin = spark.read.json(\n",
    "    \"../../assets/data/yelp_academic/yelp_academic_dataset_checkin.json.gz\"\n",
    ")\n",
    "review = spark.read.json(\n",
    "    \"../../assets/data/yelp_academic/yelp_academic_dataset_review.json.gz\"\n",
    ")\n",
    "tip = spark.read.json(\n",
    "    \"../../assets/data/yelp_academic/yelp_academic_dataset_tip.json.gz\"\n",
    ")\n",
    "user = spark.read.json(\n",
    "    \"../../assets/data/yelp_academic/yelp_academic_dataset_user.json.gz\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output\n",
    "```                                                                                \n",
    "24/12/17 01:45:32 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create temp views for the DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business.createOrReplaceTempView(\"business\")\n",
    "checkin.createOrReplaceTempView(\"checkin\")\n",
    "tip.createOrReplaceTempView(\"tip\")\n",
    "review.createOrReplaceTempView(\"review\")\n",
    "user.createOrReplaceTempView(\"user\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 0 -- Simple Query Example\n",
    "\n",
    "Get a list of users named \"Kahlil\" with the number of their reviews tagged \"funny\".\n",
    "\n",
    "- The result should have these columns (in this order):\n",
    "  - `user_id`\n",
    "  - `name`\n",
    "  - `funny`\n",
    "- The result rows do NOT need to be ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the problem by assigning populating the provided variable\n",
    "# with the result of the Spark SQL query\n",
    "\n",
    "\n",
    "def users_kahlil():\n",
    "    return spark.sql(\n",
    "        \"\"\"\\\n",
    "        SELECT user_id, name, funny\n",
    "        FROM user\n",
    "        WHERE name = \"Kahlil\"\n",
    "        \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It can be helpful to look at the result with .show()\n",
    "\n",
    "results = users_kahlil()\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output\n",
    "```\n",
    "+--------------------+------+-----+\n",
    "|             user_id|  name|funny|\n",
    "+--------------------+------+-----+\n",
    "|HE5fZW8m7MpdLHa3H...|Kahlil|   32|\n",
    "|BAX7MdujQiv_Camqi...|Kahlil|    0|\n",
    "|fepcVUPERVRA16b4M...|Kahlil|    0|\n",
    "|uvG9MAZF6vIVBoj24...|Kahlil|    4|\n",
    "|sEQtegzBDjARGB_YM...|Kahlil|    0|\n",
    "|JpOCv0TtT2nz0gv0S...|Kahlil|    0|\n",
    "+--------------------+------+-----+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook provides several asserts for each problem.\n",
    "#\n",
    "# There are also hidden tests that are run by the autograder after submission.\n",
    "\n",
    "assert callable(users_kahlil), \"The answer must be a callable function.\"\n",
    "\n",
    "result = users_kahlil()\n",
    "assert (\n",
    "    type(result) == pyspark.sql.dataframe.DataFrame\n",
    "), \"The return value should be a Spark DataFrame.\"\n",
    "\n",
    "expected_columns = [\"user_id\", \"name\", \"funny\"]\n",
    "assert set(result.columns) == set(expected_columns), \"The columns are incorrect.\"\n",
    "assert result.columns == expected_columns, \"The columns are not in the correct order.\"\n",
    "\n",
    "AutograderHelper.assert_function_calls(users_kahlil, [\"spark.sql\"])\n",
    "\n",
    "users_kahlil_ids = [r[\"user_id\"] for r in users_kahlil().collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(users_kahlil_ids) == 6, \"The result must have 6 rows.\"\n",
    "\n",
    "expected_user_id = \"HE5fZW8m7MpdLHa3HGp1FA\"\n",
    "assert (\n",
    "    expected_user_id in users_kahlil_ids\n",
    "), f'The user_id column should include \"{expected_user_id}\"'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 0: Simple Query Example\n",
    "**Task:** Get a list of users named \"Kahlil\" with the number of their reviews tagged \"funny\".\n",
    "\n",
    "**Solution:**\n",
    "```python\n",
    "def users_kahlil():\n",
    "    return spark.sql(\n",
    "        \"\"\"\\\n",
    "        SELECT user_id, name, funny\n",
    "        FROM user\n",
    "        WHERE name = \"Kahlil\"\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "# Assign the result to the variable\n",
    "result = users_kahlil()\n",
    "result.show()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **SELECT user_id, name, funny**: Selects the columns `user_id`, `name`, and `funny` from the `user` table.\n",
    "- **FROM user**: Specifies the `user` table as the source of the data.\n",
    "- **WHERE name = \"Kahlil\"**: Filters the rows to include only users named \"Kahlil\".\n",
    "\n",
    "**Thought Process:**\n",
    "1. Identify the columns needed: `user_id`, `name`, and `funny`.\n",
    "2. Filter the data to include only users named \"Kahlil\".\n",
    "3. Use `spark.sql()` to execute the query and return the result.\n",
    "\n",
    "### Summary:\n",
    "- **Question 0:** Get a list of users named \"Kahlil\" with the number of their reviews tagged \"funny\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Question 1 -- Users with 500 Fans\n",
    "\n",
    "Determine how many users have more than 500 fans.\n",
    "\n",
    "- The result should have 1 column and 1 row\n",
    "- The name of the column does not matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_users_500_fans():\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "assert callable(count_users_500_fans), \"The answer must be a callable function.\"\n",
    "\n",
    "assert (\n",
    "    type(count_users_500_fans()) == pyspark.sql.dataframe.DataFrame\n",
    "), \"The return value should be a Spark DataFrame.\"\n",
    "\n",
    "AutograderHelper.assert_function_calls(count_users_500_fans, [\"spark.sql\"])\n",
    "\n",
    "count_users_500_fans_submitted = count_users_500_fans().collect()[0][0]\n",
    "\n",
    "assert (\n",
    "    count_users_500_fans_submitted != 8286\n",
    "), \"That is the number of users who have more than 500 funny ratings.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Users with 500 Fans\n",
    "**Task:** Determine how many users have more than 500 fans.\n",
    "\n",
    "**Solution:**\n",
    "```python\n",
    "def count_users_500_fans():\n",
    "    query = \"\"\"\n",
    "    SELECT COUNT(*) AS user_count\n",
    "    FROM user\n",
    "    WHERE fans > 500\n",
    "    \"\"\"\n",
    "    return spark.sql(query)\n",
    "\n",
    "# Assign the result to the variable\n",
    "result = count_users_500_fans()\n",
    "result.show()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **SELECT COUNT(*) AS user_count**: Counts the number of users who meet the condition and assigns the result to the column `user_count`.\n",
    "- **FROM user**: Specifies the `user` table as the source of the data.\n",
    "- **WHERE fans > 500**: Filters the rows to include only users with more than 500 fans.\n",
    "\n",
    "**Thought Process:**\n",
    "1. Identify the condition: users with more than 500 fans.\n",
    "2. Use the `COUNT(*)` function to count the number of users who meet this condition.\n",
    "3. Use `spark.sql()` to execute the query and return the result.\n",
    "\n",
    "### Summary:\n",
    "- **Question 1:** Determine how many users have more than 500 fans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 -- Business Reviews\n",
    "\n",
    "Using the `business` table, determine how many businesses have at least 4 stars and at least 100 reviews.\n",
    "\n",
    "- The result should have 1 column and 1 row\n",
    "- The name of the column does not matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def business_reviews_count():\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "assert callable(business_reviews_count), \"The answer must be a callable function.\"\n",
    "\n",
    "assert (\n",
    "    type(business_reviews_count()) == pyspark.sql.dataframe.DataFrame\n",
    "), \"The return value should be a Spark DataFrame.\"\n",
    "\n",
    "AutograderHelper.assert_function_calls(business_reviews_count, [\"spark.sql\"])\n",
    "\n",
    "business_reviews_count_submitted = business_reviews_count().collect()[0][0]\n",
    "\n",
    "assert business_reviews_count_submitted != 2814, (\n",
    "    \"2814 is the number of businesses with greater than 4 stars (you should include ones with 4 stars) \"\n",
    "    \"and greater than 100 reviews (you should include ones with 100 reviews).\"\n",
    ")\n",
    "\n",
    "assert business_reviews_count_submitted != 7397, (\n",
    "    \"7397 is the number of businesses with at least 4 stars and greater than 100 reviews (you should \"\n",
    "    \"include ones with 100 reviews).\"\n",
    ")\n",
    "assert business_reviews_count_submitted != 2842, (\n",
    "    \"2842 is the number of businesses with greater than 4 stars (you should include ones with 4 stars) \"\n",
    "    \"and at least 100 reviews.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 -- Litchfield, Ohio\n",
    "\n",
    "Get a list of businesses from Litchfield, OH. \n",
    "\n",
    "- The result should have these columns (in this order):\n",
    "  - `business_id`\n",
    "  - `name`\n",
    "- The result rows do NOT need to be ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def litchfield_oh_businesses():\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "assert callable(litchfield_oh_businesses), \"The answer must be a callable function.\"\n",
    "\n",
    "AutograderHelper.assert_function_calls(litchfield_oh_businesses, [\"spark.sql\"])\n",
    "\n",
    "result = litchfield_oh_businesses()\n",
    "assert (\n",
    "    type(result) == pyspark.sql.dataframe.DataFrame\n",
    "), \"The return value should be a Spark DataFrame.\"\n",
    "\n",
    "expected_columns = [\"business_id\", \"name\"]\n",
    "assert set(result.columns) == set(expected_columns), \"The columns are incorrect.\"\n",
    "assert result.columns == expected_columns, \"The columns are not in the correct order.\"\n",
    "\n",
    "litchfield_oh_business_names = [r[\"name\"] for r in result.collect()]\n",
    "\n",
    "assert (\n",
    "    \"Tonios Pizza\" in litchfield_oh_business_names\n",
    "), \"'Tonios Pizza' should appear in the result.\"\n",
    "assert (\n",
    "    \"Hayseed\" not in litchfield_oh_business_names\n",
    "), \"'Hayseed' should not appear in the result.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 -- US States\n",
    "\n",
    "Determine which US states are represented in the data set. (The file `../../assets/data/states.csv` contains a list of US state names and abbreviations.) Part of your task is to import the full names of states and use those values in the result.\n",
    "\n",
    "- The result should have one column:\n",
    "  - `state` (the full name of the state in the dataset)\n",
    "- The result rows do NOT need to be ordered\n",
    "- We are looking for the full names of states that appear in the `business` table\n",
    "  - Hint: Michigan does not appear in that table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def states_names_in_data():\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert callable(states_names_in_data), \"The answer must be a callable function.\"\n",
    "\n",
    "AutograderHelper.assert_function_calls(states_names_in_data, [\"spark.sql\"])\n",
    "\n",
    "assert (\n",
    "    type(states_names_in_data()) == pyspark.sql.dataframe.DataFrame\n",
    "), \"The return value should be a Spark DataFrame.\"\n",
    "\n",
    "state_names_list = [r[\"state\"] for r in states_names_in_data().collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    \"North Carolina\" in state_names_list\n",
    "), \"North Carolina should appear in the result.\"\n",
    "assert \"Michigan\" not in state_names_list, \"Michigan should not appear in the result.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Query:\n",
    "```python\n",
    "query = \"\"\"\n",
    "SELECT DISTINCT s.state AS state\n",
    "FROM business b\n",
    "JOIN states s ON b.state = s.abbreviation\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "### Function:\n",
    "```python\n",
    "def states_names_in_data():\n",
    "    query = \"\"\"\n",
    "    SELECT DISTINCT s.state AS state\n",
    "    FROM business b\n",
    "    JOIN states s ON b.state = s.abbreviation\n",
    "    \"\"\"\n",
    "    return spark.sql(query)\n",
    "\n",
    "# Assign the result to the variable\n",
    "result = states_names_in_data()\n",
    "result.show()\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "- **SELECT DISTINCT s.state AS state**: Selects the unique full names of the states from the `states` table.\n",
    "- **FROM business b**: Specifies the `business` table as the source of the data.\n",
    "- **JOIN states s ON b.state = s.abbreviation**: Joins the `business` table with the `states` table on the state abbreviation.\n",
    "\n",
    "### Thought Process:\n",
    "1. Correct the column name for the state abbreviation in the `states` table.\n",
    "2. Join the `business` table with the `states` table to get the full names of the states.\n",
    "3. Select the unique full names of the states that appear in the `business` table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6 -- Most Tips\n",
    "\n",
    "Determine the names of the top 100 users who provided the most tips.\n",
    "\n",
    "- The result should have these columns (in this order):\n",
    "  - `name`\n",
    "  - `tip_count`\n",
    "- The result should be sorted by highest-to-lowest tip_count, in the case of tip_count ties, the results should be sorted by name alphabetically. For example (this is fake data):\n",
    "  ```\n",
    "  +--------+---------+\n",
    "  |    name|tip_count|\n",
    "  +--------+---------+\n",
    "  | Weifong|      167|\n",
    "  |   Alice|       42|\n",
    "  |     Bob|       42|\n",
    "  |   Jamal|        3|\n",
    "  +--------+---------+\n",
    "  ```\n",
    "  \n",
    "Note that not having the secondary sorting can cause subtle bugs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def users_top_100_tip_count():\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert callable(users_top_100_tip_count), \"The answer must be a callable function.\"\n",
    "\n",
    "AutograderHelper.assert_function_calls(users_top_100_tip_count, [\"spark.sql\"])\n",
    "\n",
    "result = users_top_100_tip_count()\n",
    "assert (\n",
    "    type(result) == pyspark.sql.dataframe.DataFrame\n",
    "), \"The return value should be a Spark DataFrame.\"\n",
    "\n",
    "expected_columns = [\"name\", \"tip_count\"]\n",
    "assert set(result.columns) == set(expected_columns), \"The columns are incorrect.\"\n",
    "assert result.columns == expected_columns, \"The columns are not in the correct order.\"\n",
    "\n",
    "users_top_100_tip_count_first_row = result.take(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    users_top_100_tip_count_first_row[\"name\"] == \"Momo\"\n",
    "), \"The first name should be Momo\"\n",
    "assert (\n",
    "    users_top_100_tip_count_first_row[\"tip_count\"] == 2439\n",
    "), \"The first tip_count should be 2439\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7 -- Arizona Summary\n",
    "\n",
    "List the names, number of reviews of businesses in Arizona ('AZ') and total number of reviews of the top 10 users (as determined by who has created the most number of reviews of businesses in Arizona). Include a column that shows the percentage of reviews that are of businesses from Arizona. \n",
    "\n",
    "- The result should have these columns (in this order):\n",
    "  - `name`\n",
    "  - `az_count`\n",
    "  - `total_count` (Use the \"user\" table's \"review_count\" column for this value)\n",
    "  - `percent` (this will only be checked to within 0.01)\n",
    "- The result should be sorted by highest-to-lowest `az_count`, in the case of `az_count ties`, the results should be sorted by highest-to-lowest `percent`\n",
    "\n",
    "\n",
    "\n",
    "The first row of the results should be:\n",
    "```\n",
    "+--------+--------+-----------+---------+\n",
    "|    name|az_count|total_count|  percent|\n",
    "+--------+--------+-----------+---------+\n",
    "|    Brad|    1637|       1642|99.695496|\n",
    "+--------+--------+-----------+---------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arizona_summary():\n",
    "    # YOUR CODE HERE\n",
    "    return \n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert callable(arizona_summary), \"The answer must be a callable function.\"\n",
    "\n",
    "AutograderHelper.assert_function_calls(arizona_summary, [\"spark.sql\"])\n",
    "\n",
    "result = arizona_summary()\n",
    "assert (\n",
    "    type(result) == pyspark.sql.dataframe.DataFrame\n",
    "), \"The return value should be a Spark DataFrame.\"\n",
    "\n",
    "expected_columns = [\"name\", \"az_count\", \"total_count\", \"percent\"]\n",
    "assert set(result.columns) == set(expected_columns), \"The columns are incorrect.\"\n",
    "assert result.columns == expected_columns, \"The columns are not in the correct order.\"\n",
    "\n",
    "arizona_summary_first_row = result.take(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert arizona_summary_first_row[\"name\"] == \"Brad\", \"The first name should be Brad\"\n",
    "assert (\n",
    "    arizona_summary_first_row[\"az_count\"] == 1637\n",
    "), \"The first az_count should be 1637\"\n",
    "assert (\n",
    "    arizona_summary_first_row[\"total_count\"] == 1642\n",
    "), \"The first total_count should be 1642\"\n",
    "\n",
    "# We use __builtin__.round() here to avoid problems if pyspark.sql.functions.round\n",
    "# is imported in to the global namespace\n",
    "assert __builtin__.round(float(arizona_summary_first_row[\"percent\"]), 2) == 99.70, (  # noqa\n",
    "    f\"The first percent should be about 99.70 (checking to \"\n",
    "    f\"nearest 0.01, found {arizona_summary_first_row['percent']})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output\n",
    "```\n",
    "[Stage 203:===================>                                     (1 + 2) / 3]\n",
    "+--------+--------+-----------+---------+\n",
    "|    name|az_count|total_count|  percent|\n",
    "+--------+--------+-----------+---------+\n",
    "|    Brad|    1637|       1642|99.695493|\n",
    "|   Karen|    1559|       2340|66.623932|\n",
    "|Jennifer|    1250|       1929|64.800415|\n",
    "|    Gabi|    1151|       1932|59.575569|\n",
    "|    Judy|    1059|       1193|88.767812|\n",
    "|Jennifer|    1059|       4190|25.274463|\n",
    "|    John|     922|          2|  46100.0|\n",
    "|  Aileen|     908|        934|97.216274|\n",
    "|    John|     882|          3|  29400.0|\n",
    "|    John|     879|          1|  87900.0|\n",
    "+--------+--------+-----------+---------+\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write the function `arizona_summary` to list the names, number of reviews of businesses in Arizona ('AZ'), and the total number of reviews of the top 10 users who have created the most number of reviews of businesses in Arizona. We'll also include a column that shows the percentage of reviews that are of businesses from Arizona.\n",
    "\n",
    "### Function:\n",
    "```python\n",
    "def arizona_summary():\n",
    "    query = \"\"\"\n",
    "    SELECT u.name, COUNT(r.review_id) AS az_count, u.review_count AS total_count,\n",
    "           ROUND((COUNT(r.review_id) / u.review_count) * 100, 6) AS percent\n",
    "    FROM review r\n",
    "    JOIN business b ON r.business_id = b.business_id\n",
    "    JOIN user u ON r.user_id = u.user_id\n",
    "    WHERE b.state = 'AZ'\n",
    "    GROUP BY u.name, u.review_count\n",
    "    ORDER BY az_count DESC, percent DESC\n",
    "    LIMIT 10\n",
    "    \"\"\"\n",
    "    return spark.sql(query)\n",
    "\n",
    "# Assign the result to the variable\n",
    "result = arizona_summary()\n",
    "result.show()\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "- **SELECT u.name, COUNT(r.review_id) AS az_count, u.review_count AS total_count, ROUND((COUNT(r.review_id) / u.review_count) * 100, 6) AS percent**: Selects the `name` column from the `user` table, counts the number of reviews of businesses in Arizona (`az_count`), gets the total number of reviews (`total_count`), and calculates the percentage of reviews that are of businesses from Arizona (`percent`).\n",
    "- **FROM review r**: Specifies the `review` table as the source of the data.\n",
    "- **JOIN business b ON r.business_id = b.business_id**: Joins the `review` table with the `business` table on the `business_id`.\n",
    "- **JOIN user u ON r.user_id = u.user_id**: Joins the `review` table with the `user` table on the `user_id`.\n",
    "- **WHERE b.state = 'AZ'**: Filters the rows to include only reviews of businesses in Arizona.\n",
    "- **GROUP BY u.name, u.review_count**: Groups the results by the `name` and `review_count` columns.\n",
    "- **ORDER BY az_count DESC, percent DESC**: Orders the results by `az_count` in descending order and by `percent` in descending order in case of ties.\n",
    "- **LIMIT 10**: Limits the result to the top 10 users.\n",
    "\n",
    "### Thought Process:\n",
    "1. Identify the columns needed: `name`, `az_count`, `total_count`, and `percent`.\n",
    "2. Join the `review` table with the `business` and `user` tables to get the necessary data.\n",
    "3. Filter the data to include only reviews of businesses in Arizona.\n",
    "4. Count the number of reviews of businesses in Arizona and get the total number of reviews for each user.\n",
    "5. Calculate the percentage of reviews that are of businesses from Arizona.\n",
    "6. Sort the results by `az_count` in descending order and by `percent` in descending order in case of ties.\n",
    "7. Limit the result to the top 10 users.\n",
    "8. Use `spark.sql()` to execute the query and return the result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
