{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of Spark DataFrames\n",
    "- DataFrames are an essential component in Spark, organized into named columns, similar to tables in Pandas, R DataFrames, and Excel.\n",
    "- Key operations include:\n",
    "    - **Selecting Columns**: Use the `select` function to choose specific columns.\n",
    "    - **Filtering Data**: Apply conditions to filter rows, similar to showing rows where, for example, a \"stars\" column has values greater than or equal to four.\n",
    "    - **Displaying Results**: Use show for a human-readable format or `collect` for a list of rows suitable for further processing.\n",
    "    - **Grouping and Sorting**: Use `groupBy` for aggregation (e.g., counting) and sort results in a desired order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Spark SQL\n",
    "- **Temporary Views**: Learn to register temporary views within Spark SQL, enabling the flexibility to use SQL queries directly on Spark DataFrames.\n",
    "- **SQL Interface**: Provides an option to perform complex queries without loading data into an external database, using familiar SQL syntax.\n",
    "- **Advanced Topics**:\n",
    "    - Reviewed previous SQL concepts to ensure understanding, as these are central to working with Spark SQL.\n",
    "    - Emphasized on chaining filters and handling complex boolean operations for more intricate queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Defined Functions (UDFs)\n",
    "- Analogous to `map` or `apply` functions in Pandas but used within Spark.\n",
    "- UDFs allow data scientists to write custom, expressive functions compatible with the Spark ecosystem.\n",
    "- Highlighted their importance as a foundational tool, essential for advanced analytical tasks in Spark SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "The week is dedicated to reinforcing foundational concepts in Spark SQL and bridging them with practical data operations. It emphasizes the analytical power of leveraging both DataFrames and SQL-type operations within Spark to handle complex data manipulation and analysis effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textbook Notes:\n",
    "1. **SAIDS 502 - Math Methods 1:**\n",
    "*There is no required textbook for this course, but there are three recommended textbooks:*\n",
    "    - `Essential Math for Data Science` by Nield \n",
    "    - `Linear Algebra and its Applications` by Lay \n",
    "    - `A First Course in Probability` by Ross \n",
    "    - `Introduction to Econometrics` by Stock and Watson \n",
    "2. **SAIDS 521 - Visual Exploration of Data:**\n",
    "    - `Chapter 4, Exploratory Data Analysis` , from `Experimental Design and Analysis`, by Howard J. Seltman. Copyright 2018 Howard Seltman.\n",
    "    - `Matplotlib 3.0 Cookbook` by Srinivasa Rao Poladi (OÊ¼Reilly). Copyright 2018 Packt Press  978-1-789-13571-8. \n",
    "3. **SIADS 532 - Data Mining I:**\n",
    "    - `Data mining: concepts and techniques`. Han, J., Pei, J. and Kamber, M. (3rd Edition) [DMCT] \n",
    "    - `Mining of massive datasets`. Leskovec, Jure, Anand Rajaraman, and Jeffrey David Ullman. (2nd Edition) [MMDS] \n",
    "4. **SAIDS 602 - Math Methods II:**\n",
    "*Textbooks for Optional Readings* \n",
    "    - `Mathematics for Machine Learning` by Marc Peter Deisenroth, A. Aldo Faisal and Cheng Soon Ong. Cambridge University Press, 2020. \n",
    "    - `Pattern Recognition and Machine Learning` by Christopher  M. Bishop. Springer-Verlag New York, 2006. \n",
    "    - `Introduction to Probability` by Dimitri P. Berts"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
