{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction and Overview\n",
    "- **Objective**:\n",
    "    - Review Spark APIs covered in previous weeks.\n",
    "    - Introduce Spark DataFrames focusing on three key aspects:\n",
    "        - Creation\n",
    "        - Manipulation\n",
    "        - User-Defined Functions (UDFs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of Spark APIs\n",
    "\n",
    "**Resilient Distributed Datasets (RDDs)**\n",
    "\n",
    "- **Definition**:\n",
    "    - RDDs are the foundational data structure in Spark, representing a distributed collection of data.\n",
    "- **Characteristics**:\n",
    "    -**Low-level AP**: Requires explicit manipulation.\n",
    "    - **Supports**:\n",
    "        - **Creation**:\n",
    "            - `parallelize` existing data structures.\n",
    "            - `textFile` to load data from files.\n",
    "        - **Transformations**:\n",
    "            - Examples include `map`, `reduce`, `reduceByKey`, `sortByKey`.\n",
    "        - **Actions**:\n",
    "            - Return non-RDD results, e.g., `count`, `collect`.\n",
    "- **Usage Recap**:\n",
    "    - Filter lines containing specific keywords.\n",
    "    - Apply transformations like squaring numbers using `map`.\n",
    "    - Perform actions to retrieve results, keeping in mind performance implications with large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Datasets\n",
    "\n",
    "- **Definition**:\n",
    "    - Distributed collections of data, similar to DataFrames but available only in Scala and Java.\n",
    "- **Limitations**:\n",
    "    - No Python interface.\n",
    "- **Additional Notes**:\n",
    "    - Spark is written in Scala, which runs on the JVM, leading to certain functionalities being exclusive to Scala and Java.\n",
    "    - Error messages may reference Java due to Spark's underlying implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark DataFrames\n",
    "- **Definition**:\n",
    "    - Structured as distributed collections of data organized into named columns, analogous to tables in Pandas or R.\n",
    "- **Availability**:\n",
    "    - Accessible via Python, Scala, and Java (excluding Python for Datasets)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-Depth Discussion on Spark DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From a List\n",
    "- **Method**:\n",
    "    - Use `createDataFrame` function with a list of tuples and a list of column names.\n",
    "- **Example**:\n",
    "    - Data: `[(\"Chris\", 67), (\"Frank\", 70)]`\n",
    "    - Columns: `[\"name\", \"score\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output   \n",
    "+-----+-----+\n",
    "| name|score|\n",
    "+-----+-----+\n",
    "|Chris|   67|\n",
    "|Frank|   70|\n",
    "+-----+-----+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From an RDD\n",
    "- **Method**:\n",
    "    - Convert an existing RDD to a DataFrame.\n",
    "    - Default column names appear as `_1`, `_2`, etc., unless explicitly defined.\n",
    "- **Enhancement**:\n",
    "    - Use `Row` objects to assign meaningful column names during conversion.\n",
    "- **Example**:\n",
    "    - Original RDD: `[(\"Chris\", 67), (\"Frank\", 70)]`\n",
    "    - Transformed with `Row(name=x[0], score=int(x[1]))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output with Named Columns\n",
    "+-----+-----+\n",
    "| name|score|\n",
    "+-----+-----+\n",
    "|Chris|   67|\n",
    "|Frank|   70|\n",
    "+-----+-----+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From a JSON File\n",
    "- **Method**:\n",
    "    - Utilize `spark.read.json(\"filename.json\")` to create a DataFrame.\n",
    "- **Advantages**:\n",
    "    - Automatic schema inference.\n",
    "- **Complexity**:\n",
    "    - JSON files can have nested structures which Spark handles efficiently.\n",
    "- **Schema Example**: `root`\n",
    " |`-- addresses: string (nullable = true)`\n",
    " |`-- attributes: struct (nullable = true)`\n",
    " |`    |-- ambiance: struct (nullable = true)`\n",
    " |`    |    |-- casual: string (nullable = true)`\n",
    " |`    |    |-- intimate: string (nullable = true)`\n",
    " |`-- is_open: boolean (nullable = true)`\n",
    " |`-- latitude: double (nullable = true)`\n",
    " |`-- longitude: double (nullable = true)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulation\n",
    "- **Primary Functions**:\n",
    "    - `show()`: Displays the first 20 entries by default.\n",
    "    - `first()`: Retrieves the first row.\n",
    "    - `head(n)`: Returns the first n rows as a list.\n",
    "    - `take(n)`: Similar to head, retrieves the first `n` rows.\n",
    "    - `collect()`: Returns all rows as a Python list (use with caution on large datasets).\n",
    "- **Best Practices**:\n",
    "    - Use `collect()` sparingly to avoid memory issues with large datasets.\n",
    "    - Prefer actions like `show`, `head`, or `take` for exploratory data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-Defined Functions (UDFs)\n",
    "- **Upcoming Content**:\n",
    "    - Detailed exploration of creating and applying UDFs on DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of RDDs (from Last Week)\n",
    "- **Pattern**:\n",
    "    - Create a Spark session using the Builder pattern.\n",
    "    - Utilize SparkContext for RDD operations.\n",
    "- **Example Workflow**:\n",
    "    1. Create an RDD:\n",
    "        - `data_lines = sc.textFile(\"data.txt\")`\n",
    "    2. **Filter and Transform**:\n",
    "        - Filter lines containing the keyword \"data\".\n",
    "        - Apply map functions to transform data (e.g., squaring numbers).\n",
    "    3. **Actions**:\n",
    "        - Use `collect()` to retrieve the transformed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples and Demonstrations\n",
    "- **Jupyter Notebook Usage**:\n",
    "    - Demonstrated creating Spark sessions and DataFrames.\n",
    "    - Showcased various DataFrame creation methods and their outputs.\n",
    "    - Highlighted the importance of specifying data types explicitly in Spark.\n",
    "- **Code Highlights**:\n",
    "    - Importing necessary functions: from `pyspark.sql.types` `import` `FloatType`\n",
    "    - Creating DataFrames with specified schemas to ensure data type consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Items\n",
    "- **Practice DataFrame Creation**:\n",
    "    - Create DataFrames from different sources: lists, RDDs, JSON files.\n",
    "    - Experiment with specifying column names and data types.\n",
    "- **Explore Data Manipulation Functions**:\n",
    "    - Use `show`, `first`, `head`, `take`, and `collect` on various DataFrames.\n",
    "    - Observe the differences in outputs and performance implications.\n",
    "- **Prepare for UDFs**:\n",
    "    - Start thinking about potential use cases for user-defined functions in data manipulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow-up\n",
    "- **Next Session**:\n",
    "    - Dive deeper into DataFrame manipulation and transformations.\n",
    "    - Introduction to User-Defined Functions (UDFs) and their applications.\n",
    "- **Resources**:\n",
    "    - Review Spark documentation on DataFrames and RDDs.\n",
    "    - Explore Scala basics for understanding Spark's underlying architecture (optional)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This week provided a comprehensive overview of Spark DataFrames, including their creation from various sources, manipulation techniques, and a foundational understanding of how they differ from RDDs. Emphasis was placed on practical examples using Jupyter notebooks to solidify the concepts discussed."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
