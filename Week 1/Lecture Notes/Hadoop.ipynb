{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction to Big Data and Hadoop**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The discussion centers on building understanding around handling big data.\n",
    "- After exploring concepts like MapReduce and distributed computing, the conversation shifts focus to Hadoop, a framework enabling horizontal scaling.\n",
    "- Hadoop functions as an ecosystem comprising various software packages for storage and computation across distributed machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hadoop’s Role and Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hadoop's primary role as a storage solution is emphasized due to advancements in computational methods.\n",
    "- Comprises four main modules:\n",
    "    1. Hadoop Common: Core module providing essential services.\n",
    "    2. HDFS (Hadoop Distributed File System): Distributes data across commodity machines to optimize data and computation proximity.\n",
    "    3. YARN (Yet Another Resource Negotiator): Manages resources, ensures fault tolerance, and maintains task continuity in case of machine failures.\n",
    "    4. MapReduce: Java-based system for parallel computing, with a Python interface used in the discussed course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practical Example of Hadoop Utilization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A university uses library terminal machines as a Hadoop cluster for computational tasks during off-hours, such as performing optical character recognition on scanned texts.\n",
    "- Demonstrates the cost-effectiveness and efficiency of using existing resources, highlighting Hadoop's real-world application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Detailed Overview of HDFS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- HDFS focuses on bringing data close to where computation happens to avoid network bottlenecks, a crucial factor for effective analytics.\n",
    "- Typical block sizes distributed across a cluster range from 64 to 256 megabytes, emphasizing the system's scalability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Future Course Directions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The course will initially cover Hadoop before moving onto Spark, which operates on top of Hadoop infrastructure.\n",
    "- Some auxiliary components like Storm and Hive will not be primary focuses; instead, SQL-based operations on Spark will be prioritized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Additional Discussions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The concept of scale and data distribution was touched upon, suggesting computational frameworks can manage significant data volumes efficiently.\n",
    "- Although big data will not always be the focus of analysis, understanding how to maximize Hadoop's capabilities remains critical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Action Items and Follow-Ups**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Integrate knowledge of Hadoop’s components and their interrelations in practice.\n",
    "- Anticipate deeper engagement with Spark and its interaction with Hadoop for enhanced data processing capabilities in subsequent sessions."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
