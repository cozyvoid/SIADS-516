{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 0 -- Word Count\n",
    "\n",
    "### Our first mrjob script\n",
    "\n",
    "Recall the following example from the lectures:\n",
    "\n",
    "Note the use of the magic command ```%%file```.  You can use this to write the contents of a cell out to a file, which is what we need to do to use mrjob:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up previous versions of word_count.py and its output files\n",
    "!rm -f -v word_count*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file word_count.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "import re\n",
    "\n",
    "class MRWordFrequencyCount(MRJob):\n",
    "\n",
    "  ### input: self, in_key, in_value\n",
    "  def mapper(self, _, line):\n",
    "    yield \"chars\", len(line)\n",
    "    yield \"words\", len(line.split())\n",
    "    yield \"lines\", 1\n",
    "\n",
    "  ### input: self, in_key from mapper, in_value from mapper\n",
    "  def reducer(self, key, values):\n",
    "    yield key, sum(values)\n",
    "if __name__ == \"__main__\":\n",
    "    MRWordFrequencyCount.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the output of running the script with that file. Note that we are using the `tee` command here to make things easier for the autograder. The `|` character sends the output of our script to the `tee` command which prints it to the display, and also writes the script output to the given file name (\"`file_stats_output.tsv`\" in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python word_count.py ../../assets/data/gutenberg/short.t1.txt | tee word_count_output_short.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "````\n",
    "# Output\n",
    "\n",
    "No configs found; falling back on auto-configuration\n",
    "No configs specified for inline runner\n",
    "Creating temp directory /tmp/word_count.jovyan.20241125.023615.059714\n",
    "Running step 1 of 1...\n",
    "job output is in /tmp/word_count.jovyan.20241125.023615.059714/output\n",
    "Streaming final output from /tmp/word_count.jovyan.20241125.023615.059714/output...\n",
    "\"words\"\t1822\n",
    "\"lines\"\t200\n",
    "\"chars\"\t10653\n",
    "Removing temp directory /tmp/word_count.jovyan.20241125.023615.059714...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A note on interpreting MRJob output...\n",
    "\n",
    "Since we're using the tee command above, the standard output (`stdout`) from the `MRJob` command (`python word_count.py`) is redirected to the file `word_count_output_short.tsv`, and that is what the grader cell below is reading. This is why the grader cells that show answers are only showing the data output. The data lines for the output above should be:\n",
    "\n",
    "- `\"chars\"`\t`10653`\n",
    "- `\"lines\"`\t`200`\n",
    "- `\"words\"`\t`1822`\n",
    "\n",
    "The other output before and after the data lines are logging messages from `MRJob`. These are written to standard error (`stderr`) and are visible in the notebook, but not in the data output saved to the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell deliberately includes answers to provide guidance on how questions in this assignment are graded.\n",
    "\n",
    "correct = AutograderHelper.parse_mrjob_output(\n",
    "    \"\"\"\n",
    "\"chars\"\t10653\n",
    "\"lines\"\t200\n",
    "\"words\"\t1822\n",
    "\"\"\".strip().split(\"\\n\")\n",
    ")\n",
    "\n",
    "submitted = AutograderHelper.parse_mrjob_output_file(\"word_count_output_short.tsv\")\n",
    "\n",
    "AutograderHelper.assert_same_shape(correct, submitted)\n",
    "AutograderHelper.assert_same_rows(correct, submitted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 1 -- Most-used Word\n",
    "\n",
    "A note about word splitting:\n",
    "\n",
    "The `split()` method used in the example above, breaks on white space and there are cases where this isn't ideal:\n",
    "\n",
    "```python\n",
    "\"My dog--Luna--is barking\".split()         # --> ['My', 'dog--Luna--is', 'barking']\n",
    "```\n",
    "\n",
    "So in the following exercises, we will use a regular expression to split words instead. Note that the starter code below provides a function `splitter()`. We can use this to get better word splitting:\n",
    "\n",
    "```python\n",
    "splitter(\"My dog--Luna--is barking\")       # --> ['My', 'dog', 'Luna', 'is', 'barking']\n",
    "```\n",
    "\n",
    "### Task: Complete the most-used word count implementation\n",
    "\n",
    "Your task in this exercise is to complete the implementation of the `mapper_get_words()` method below. It should:\n",
    "- Use `splitter()` function to split words\n",
    "- Yield 2-tuples that are key-value pairs, where the key (first item in the tuple) is the word to be counted.\n",
    "- Only yield for words that are NOT in the STOPWORDS set\n",
    "- This should be case insensitive, meaning that, for example, \"Dog\" and \"dog\" will be processed together.\n",
    "\n",
    "Note that you should only need to modify the `mapper_get_words` method.\n",
    "\n",
    "**A note about debugging in this context...**\n",
    "\n",
    "- **Use `debug()` instead of `print()`** Since we are running these MRJob scripts and capturing their output for grading, if you use the standard print() function for debugging, your message will end up in the data output which will corrupt the results. So instead use the debug() function that is defined at the top of the starter code for each of the scripts below. It works mostly like print() does, but its output goes to the stderr interface instead of stdout. This means you'll see it like a normal print() in the output of a notebook cell, but those message won't end up in the data output.\n",
    "- **Double check the data output** If you are getting unexpected results from the grader, one thing to double check is the output date file directly. For example, the first output file by this notebook is \"word_count_output_short.tsv\" (above). You can open that file and make sure its contents are what you would expect (in that case three lines with the keys \"chars\", \"lines\", and \"words\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up previous versions of most_used_word.py and its output files\n",
    "!rm -f -v most_used_word*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#Output\n",
    "removed 'top_10_syllable_count_output_short.tsv'\n",
    "removed 'top_10_syllable_count.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file most_used_word.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "\n",
    "from sys import stderr\n",
    "\n",
    "# See the note above about debugging\n",
    "def debug(*msg, **kwargs):\n",
    "    \"\"\"Print debugging message to standard error.\"\"\"\n",
    "    print(*msg, file=stderr, **kwargs)\n",
    "    \n",
    "    \n",
    "def splitter(text):\n",
    "    WORD_RE = re.compile(r\"[\\w']+\")\n",
    "    return WORD_RE.findall(text)\n",
    "\n",
    "\n",
    "STOPWORDS = {\n",
    "    'i', 'we', 'ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during',\n",
    "    'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such',\n",
    "    'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each',\n",
    "    'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me',\n",
    "    'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up',\n",
    "    'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been',\n",
    "    'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so',\n",
    "    'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself',\n",
    "    'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by',\n",
    "    'doing', 'it', 'how', 'further', 'was', 'here', 'than'\n",
    "}\n",
    "\n",
    "\n",
    "class MRMostUsedWord(MRJob):    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "                   reducer=self.reducer_count_words),\n",
    "            MRStep(reducer=self.reducer_find_max_word)\n",
    "        ]\n",
    "\n",
    "    def mapper_get_words(self, _, line):\n",
    "        # Use splitter to split the line into words\n",
    "        words = splitter(line)\n",
    "        # raise NotImplementedError()\n",
    "\n",
    "        # Yield words that are not in the STOPWORDS set\n",
    "        for word in words:\n",
    "            word = word.lower()     # Convert to lowercase for case insensitivity\n",
    "            if word not in STOPWORDS:\n",
    "                yield (word, 1)\n",
    "        \n",
    "    def reducer_count_words(self, word, counts):\n",
    "        # send all (num_occurrences, word) pairs to the same reducer.\n",
    "        # num_occurrences is used so we can easily use Python's max() function.\n",
    "        yield None, (sum(counts), word)\n",
    "\n",
    "    # discard the key; it is just None\n",
    "    def reducer_find_max_word(self, _, word_count_pairs):\n",
    "        # each item of word_count_pairs is (count, word),\n",
    "        # so yielding one results in key=counts, value=word\n",
    "        yield max(word_count_pairs)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import time\n",
    "    start = time.time()\n",
    "    MRMostUsedWord.run()\n",
    "    end = time.time()\n",
    "    debug(\"Run time:\", end - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "1. `splitter` Function: Splits the text into words using a regular expression.\n",
    "\n",
    "2. `STOPWORDS` Set: Contains common stopwords to be filtered out.\n",
    "\n",
    "3. `mapper_get_words` Method:\n",
    "    - Uses `splitter` to split the line into words.   \n",
    "    - Converts each word to lowercase to ensure case insensitivity.\n",
    "    - Filters out stopwords and yields the word with a count of 1.\n",
    "\n",
    "Save this script as `most_used_word.py`, then run it using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python most_used_word.py ../../assets/data/gutenberg/t3.lewis.txt | tee most_used_word_output_lewis.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will execute the `most_used_word.py` script on `t3.lewis.txt` and save the output to `most_used_word_output_lewis.tsv`, while also displaying the results in the terminal.\n",
    "\n",
    "Once you've run the command, you can check the contents of `most_used_word_output_lewis.tsv` to see the most used word in the file along with its count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "```\n",
    "# Ouput\n",
    "No configs found; falling back on auto-configuration\n",
    "No configs specified for inline runner\n",
    "Creating temp directory /tmp/most_used_word.jovyan.20241125.032018.945038\n",
    "Running step 1 of 2...\n",
    "Running step 2 of 2...\n",
    "job output is in /tmp/most_used_word.jovyan.20241125.032018.945038/output\n",
    "Streaming final output from /tmp/most_used_word.jovyan.20241125.032018.945038/output...\n",
    "1334\t\"river\"\n",
    "Removing temp directory /tmp/most_used_word.jovyan.20241125.032018.945038...\n",
    "Run time: 1.8390789031982422 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! It looks like the script ran successfully and identified the most used word in `t3.lewis.txt` as \"**river**\" with a count of **1334**.\n",
    "\n",
    "**Summary of Output**\n",
    "- **Most Used Word**: \"river\"\n",
    "- **Count**: 1334\n",
    "- **Run Time**: Approximately 1.84 seconds\n",
    "\n",
    "**Clean Output**\n",
    "The result indicates that the word \"river\" appears 1334 times in the text file `t3.lewis.txt`. This output is captured and displayed by the `tee` command, and the script executed efficiently within a reasonable time frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autograder cell. This cell is worth 1 point (out of 20). This cell does not contain hidden tests.\n",
    "# This cell deliberately includes answers to provide guidance on how this question is graded.\n",
    "\n",
    "correct = AutograderHelper.parse_mrjob_output(\n",
    "    \"\"\"\n",
    "1334\t\"river\"\n",
    "\"\"\".strip().split(\"\\n\")\n",
    ")\n",
    "\n",
    "submitted = AutograderHelper.parse_mrjob_output_file(\"most_used_word_output_lewis.tsv\")\n",
    "\n",
    "AutograderHelper.assert_same_shape(correct, submitted)\n",
    "AutograderHelper.assert_same_rows(correct, submitted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the autograder cell is set up to verify that the output of your `most_used_word.py` script matches the expected output. Here's what the code does:\n",
    "\n",
    "1. **Correct Output**:\n",
    "- It defines the expected output as `1334` \"`river`\" and parses it using `AutograderHelper.parse_mrjob_output`.\n",
    "2. **Submitted Output**:\n",
    "- It reads and parses the output from `most_used_word_output_lewis.tsv` using `AutograderHelper.parse_mrjob_output_file`.\n",
    "3. **Assertions**:\n",
    "- `AutograderHelper.assert_same_shape(correct, submitted)`: Ensures that the shape (number of rows and columns) of the correct and submitted outputs match.\n",
    "- `AutograderHelper.assert_same_rows(correct, submitted)`: Ensures that each row in the correct output matches the corresponding row in the submitted output.\n",
    "\n",
    "If your script's output file most_used_word_output_lewis.tsv contains the line 1334 \"river\", then the assertions should pass, confirming that the output is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on your description, the output you obtained from running the script mathes the expected output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1334    \"river\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run this script on a larger file..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python most_used_word.py ../../assets/data/gutenberg/t8.shakespeare.txt | tee most_used_word_output_shakespeare.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will execute the `most_used_word.py` script on `t8.shakespeare.txt` and save the output to `most_used_word_output_shakespeare.tsv` while displaying the results in the terminal.\n",
    "\n",
    "Running the script on a larger file will help us see how it handles more data and identify the most used word in a different text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "```\n",
    "# Output\n",
    "No configs found; falling back on auto-configuration\n",
    "No configs specified for inline runner\n",
    "Creating temp directory /tmp/most_used_word.jovyan.20241125.032032.418489\n",
    "Running step 1 of 2...\n",
    "Running step 2 of 2...\n",
    "job output is in /tmp/most_used_word.jovyan.20241125.032032.418489/output\n",
    "Streaming final output from /tmp/most_used_word.jovyan.20241125.032032.418489/output...\n",
    "5479\t\"thou\"\n",
    "Removing temp directory /tmp/most_used_word.jovyan.20241125.032032.418489...\n",
    "Run time: 4.337578058242798 seconds \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script ran successfully on the larger file `t8.shakespeare.txt` and identified the most used word as \"**thou**\" with a count of **5479**.\n",
    "\n",
    "**Summary of Output**\n",
    "- **Most Used Word**: \"thou\"\n",
    "- **Count**: 5479\n",
    "- **Run Time**: Approximately 4.34 seconds\n",
    "\n",
    "**Clean Output**\n",
    "This result indicates that the word \"thou\" appears 5479 times in the text file `t8.shakespeare.txt`. The output was displayed in the terminal and saved to the file `most_used_word_output_shakespeare.tsv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submitted = AutograderHelper.parse_mrjob_output_file(\n",
    "    \"most_used_word_output_shakespeare.tsv\"\n",
    ")\n",
    "\n",
    "assert len(submitted) == 1, \"The submission is not the correct length.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using an assertion to verify that the submission has the correct length. Given that the output from running `most_used_word.py` on `t8.shakespeare.txt` was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output\n",
    "5479    \"thou\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output indicates that the most used word is \"thou\" with a count of 5479, and the result was saved in `most_used_word_output_shakespeare.tsv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assertion statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(submitted) == 1, \"The submission is not the correct length.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is checking if the `submitted` output contains exactly one entry. Since our output for `most_used_word_output_shakespeare.tsv` is a single line indicating the word \"thou\" and its count, the assertion should pass, confirming that the submission is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autograder cell. This cell is worth 5 points (out of 20). This cell contains hidden tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 2 -- Syllable Count\n",
    "\n",
    "The [`syllables` package](https://pypi.org/project/syllables/) (which is pre-installed for you) has an `estimate()` method you can use to get an estimated count of syllables for a given word.\n",
    "\n",
    "A couple of examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syllables\n",
    "\n",
    "syllables.estimate(\"funny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syllables.estimate(\"strengths\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are just estimates, so you'll see inaccurate counts from this package, for example, with \"temperature\". Don't worry about this. For this exercise, we just care about the result from `syllables.estimate()`, not how accurate it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syllables.estimate(\"temperature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also note that this function is sensitive to the case of the input, and can return different counts based on capitalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syllables.estimate(\"Unfortunately\")  # Note the upper-case \"U\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syllables.estimate(\"unfortunately\")  # Note the lower-case \"u\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Write an MRJob script\n",
    "\n",
    "Your task is to write a MRJob script that finds the 10 words that have the most syllables from the input file. This top-ten list should be sorted first by the syllable count, then by the word in alphabetic order. It should:\n",
    "\n",
    "- Use the `splitter()` function to split words\n",
    "- Only process words that are NOT in the STOPWORDS set\n",
    "- This should be case insensitive, meaning that, for example, \"Dog\" and \"dog\" should be processed together.\n",
    "- Use the `sort_results()` function (see below) to sort the final results.\n",
    "\n",
    "**Sorting the Results** In order to simplify interpretation of the results, use the provided `sort_results()` function. For example: if our input is:\n",
    "\n",
    "```\n",
    "The dog sleeps by the fireplace.\n",
    "```\n",
    "\n",
    "and our mapper gives us a result like this:\n",
    "\n",
    "```\n",
    "[\n",
    "    (1, \"dog\"),\n",
    "    (3, \"fireplace\"),\n",
    "    (1, \"the\"),\n",
    "    (1, \"sleeps\"),\n",
    "    (1, \"by\"),\n",
    "]\n",
    "```\n",
    "\n",
    "The `sort_results()` function will sort that like this:\n",
    "\n",
    "```\n",
    "[\n",
    "    (3, \"fireplace\"),\n",
    "    (1, \"by\"),\n",
    "    (1, \"dog\"),\n",
    "    (1, \"sleeps\"),\n",
    "    (1, \"the\"),\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up previous versions of top_10_syllable_count.py and its output files\n",
    "!rm -f -v top_10_syllable_count*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file top_10_syllable_count.py\n",
    "\n",
    "import re\n",
    "from sys import stderr\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "import syllables\n",
    "\n",
    "def debug(*msg, **kwargs):\n",
    "    \"\"\"Print debugging message to standard error.\"\"\"\n",
    "    print(*msg, file=stderr, **kwargs)\n",
    "\n",
    "def splitter(text):\n",
    "    WORD_RE = re.compile(r\"[\\w']+\")\n",
    "    return WORD_RE.findall(text)\n",
    "\n",
    "def sort_results(results):\n",
    "    \"\"\"\n",
    "    Sorts a list of 2-tuples descending by the first value in the \n",
    "    tuple, ascending by the second value in the tuple.\n",
    "    \"\"\"\n",
    "    return sorted(results, key=lambda k: (-k[0], k[1]))\n",
    "\n",
    "STOPWORDS = {\n",
    "    'i', 'we', 'ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during',\n",
    "    'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such',\n",
    "    'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each',\n",
    "    'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me',\n",
    "    'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up',\n",
    "    'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been',\n",
    "    'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so',\n",
    "    'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself',\n",
    "    'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by',\n",
    "    'doing', 'it', 'how', 'further', 'was', 'here', 'than'\n",
    "}\n",
    "\n",
    "class MRMostSyllables(MRJob):\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "                   reducer=self.reducer_find_top_ten)\n",
    "        ]\n",
    "\n",
    "    def mapper_get_words(self, _, line):\n",
    "        words = splitter(line)\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            if word.isalpha() and word not in STOPWORDS:\n",
    "                syllable_count = syllables.estimate(word)\n",
    "                yield None, (syllable_count, word)\n",
    "\n",
    "    def reducer_find_top_ten(self, _, syllable_word_pairs):\n",
    "        unique_results = {tuple(pair) for pair in syllable_word_pairs}  # Use a set of tuples to remove duplicates\n",
    "        sorted_results = sort_results(unique_results)\n",
    "        for count, word in sorted_results[:10]:\n",
    "            yield count, word\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRMostSyllables.run()\n",
    "    \n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "`````\n",
    "Overwriting top_10_syllable_count.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run this script on a small file..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python top_10_syllable_count.py ../../assets/data/gutenberg/short.t1.txt | tee top_10_syllable_count_output_short.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "````\n",
    "## Output\n",
    "\n",
    "No configs found; falling back on auto-configuration\n",
    "No configs specified for inline runner\n",
    "Creating temp directory /tmp/top_10_syllable_count.jovyan.20241125.052240.574336\n",
    "Running step 1 of 1...\n",
    "job output is in /tmp/top_10_syllable_count.jovyan.20241125.052240.574336/output\n",
    "Streaming final output from /tmp/top_10_syllable_count.jovyan.20241125.052240.574336/output...\n",
    "6\t\"phonotelephote\"\n",
    "6\t\"plenipotentiaries\"\n",
    "6\t\"revolutionized\"\n",
    "6\t\"unfortunately\"\n",
    "6\t\"unimaginable\"\n",
    "5\t\"accumulator\"\n",
    "5\t\"accumulators\"\n",
    "5\t\"agriculture\"\n",
    "5\t\"civilization\"\n",
    "5\t\"communicate\"\n",
    "Removing temp directory /tmp/top_10_syllable_count.jovyan.20241125.052240.574336...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run this script on a small file..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python top_10_syllable_count.py ../../assets/data/gutenberg/short.t1.txt | tee top_10_syllable_count_output_short.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "```\n",
    "# Output\n",
    "No configs found; falling back on auto-configuration\n",
    "No configs specified for inline runner\n",
    "Creating temp directory /tmp/top_10_syllable_count.jovyan.20241125.052242.770588\n",
    "Running step 1 of 1...\n",
    "job output is in /tmp/top_10_syllable_count.jovyan.20241125.052242.770588/output\n",
    "Streaming final output from /tmp/top_10_syllable_count.jovyan.20241125.052242.770588/output...\n",
    "6\t\"phonotelephote\"\n",
    "6\t\"plenipotentiaries\"\n",
    "6\t\"revolutionized\"\n",
    "6\t\"unfortunately\"\n",
    "6\t\"unimaginable\"\n",
    "5\t\"accumulator\"\n",
    "5\t\"accumulators\"\n",
    "5\t\"agriculture\"\n",
    "5\t\"civilization\"\n",
    "5\t\"communicate\"\n",
    "Removing temp directory /tmp/top_10_syllable_count.jovyan.20241125.052242.770588...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autograder cell. This cell is worth 2 points (out of 20). This cell does not contain hidden tests.\n",
    "# This cell deliberately includes answers to provide guidance on how this question is graded.\n",
    "\n",
    "correct = AutograderHelper.parse_mrjob_output(\n",
    "    \"\"\"\n",
    "6\t\"phonotelephote\"\n",
    "6\t\"plenipotentiaries\"\n",
    "6\t\"revolutionized\"\n",
    "6\t\"unfortunately\"\n",
    "6\t\"unimaginable\"\n",
    "5\t\"accumulator\"\n",
    "5\t\"accumulators\"\n",
    "5\t\"agriculture\"\n",
    "5\t\"civilization\"\n",
    "5\t\"communicate\"\n",
    "\"\"\".strip().split(\"\\n\")\n",
    ")\n",
    "\n",
    "submitted = AutograderHelper.parse_mrjob_output_file(\n",
    "    \"top_10_syllable_count_output_short.tsv\"\n",
    ")\n",
    "\n",
    "AutograderHelper.assert_same_shape(correct, submitted)\n",
    "AutograderHelper.assert_same_rows(correct, submitted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The autograder cell is set up to verify that the output of your `top_10_syllable_count.py` script matches the expected output. Here's what it does:\n",
    "\n",
    "1. **Correct Output**:\n",
    "- Defines the expected output with specific words and their syllable counts.\n",
    "- Parses the correct output using `AutograderHelper.parse_mrjob_output`.\n",
    "2. **Submitted Output**:\n",
    "- Reads and parses the output from `top_10_syllable_count_output_short.tsv` using `AutograderHelper.parse_mrjob_output_file`.\n",
    "3. **Assertions**:\n",
    "- `AutograderHelper.assert_same_shape(correct, submitted)`: Checks if the shapes (number of rows and columns) of the correct and submitted outputs match.\n",
    "- `AutograderHelper.assert_same_rows(correct, submitted)`: Checks if each row in the correct output matches the corresponding row in the submitted output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run this script on a larger file..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python top_10_syllable_count.py ../../assets/data/gutenberg/t5.churchill.txt | tee top_10_syllable_count_output_churchill.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "``` \n",
    "#Output\n",
    "No configs found; falling back on auto-configuration\n",
    "No configs specified for inline runner\n",
    "Creating temp directory /tmp/top_10_syllable_count.jovyan.20241125.052246.357588\n",
    "Running step 1 of 1...\n",
    "job output is in /tmp/top_10_syllable_count.jovyan.20241125.052246.357588/output\n",
    "Streaming final output from /tmp/top_10_syllable_count.jovyan.20241125.052246.357588/output...\n",
    "8\t\"incommunicability\"\n",
    "8\t\"overcapitalization\"\n",
    "7\t\"apologetically\"\n",
    "7\t\"authoritatively\"\n",
    "7\t\"characteristically\"\n",
    "7\t\"communicativeness\"\n",
    "7\t\"corroboratively\"\n",
    "7\t\"disproportionately\"\n",
    "7\t\"imaginatively\"\n",
    "7\t\"impenetrability\"\n",
    "Removing temp directory /tmp/top_10_syllable_count.jovyan.20241125.052246.357588...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Top 10 Words by Syllable Count**\n",
    "1. **incommunicability** - 8 syllables\n",
    "2. **overcapitalization** - 8 syllables\n",
    "3. **apologetically** - 7 syllables\n",
    "4. **authoritatively** - 7 syllables\n",
    "5. **characteristically** - 7 syllables\n",
    "6. **communicativeness** - 7 syllables\n",
    "7. **corroboratively** - 7 syllables\n",
    "8. **disproportionately** - 7 syllables\n",
    "9. **imaginatively** - 7 syllables\n",
    "10. **impenetrability** - 7 syllables\n",
    "\n",
    "**Summary**\n",
    "- **Run Time**: Successfully executed the task.\n",
    "- **Results**: The output correctly identifies and lists the words with the highest syllable counts, sorted appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command will execute the `top_10_syllable_count.py` script on the `t5.churchill.txt` file and save the output to `top_10_syllable_count_output_churchill.tsv`, while also displaying the results in the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submitted = AutograderHelper.parse_mrjob_output_file(\n",
    "    \"top_10_syllable_count_output_churchill.tsv\"\n",
    ")\n",
    "\n",
    "assert len(submitted) == 10, \"The submission is not the correct length.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autograder cell. This cell is worth 12 points (out of 20). This cell contains hidden tests."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
