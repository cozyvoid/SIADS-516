{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Frame Attributes vs. Functions\n",
    "- **Attributes**: Accessed without parentheses. Example: `df.columns` returns column names.\n",
    "- **Functions**: Require parentheses. Examples include `df.describe()`, `df.show()`, and `df.count()`.\n",
    "- **Distinction**: Functions perform operations and return new data frames, while attributes provide metadata about the data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Column Names and Data Types\n",
    "- **Accessing Columns**: Use `df.columns` to retrieve a list of column names.\n",
    "- **Data Types**: `df.dtypes` displays the data type for each column.\n",
    "- **Schema Inspection**: `df.printSchema()` provides a detailed structure of the data frame, including nested columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Descriptive Statistics with `describe` and `show`\n",
    "- `df.describe()`: Calculates simple statistics (`count`, `mean`, `standard deviation`, `min`, `max`) for numeric columns.\n",
    "- `df.describe().show()`: Displays the calculated statistics in a readable format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Selecting and Manipulating Columns\n",
    "- **Selecting Columns**: `df.select(\"column_name\")` extracts one or more columns, returning a new data frame.\n",
    "- **Comparison with SQL**: Upcoming discussions will differentiate PySpark's `select` from SQL's `SELECT` statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Creating and Dropping Columns\n",
    "- **Creating Columns**:\n",
    "    - Use `df.withColumn(\"new_column\", function)` to add a new column.\n",
    "    - Example: Creating a new column by applying a function from pyspark.sql.functions.\n",
    "- **Dropping Columns**: `df.drop(\"column_name\")` removes a specified column from the data frame.\n",
    "- **Immutability of RDDs**: All transformations return new data frames; original data remains unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Filtering Data Frames\n",
    "- **Filtering Rows**: `df.filter(condition)` filters rows based on a boolean condition, similar to Pandas' boolean masking.\n",
    "- **Example Condition**: Filtering rows where the `stars` column is greater than or equal to 4.\n",
    "- **Displaying Results**: Use `df.filter(condition).show()` to view filtered data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Collecting and Iterating Over Rows\n",
    "- **Collecting Data**: `df.collect()` retrieves all rows as a list of SparkRow objects.\n",
    "- **Accessing Rows**: Access individual rows using list indices, e.g., `result[0]`.\n",
    "- **Converting to Dictionary**: `row.asDict()` transforms a SparkRow into a Python dictionary for easier data manipulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Grouping and Sorting Data\n",
    "- **Group By**: `df.groupBy(\"column\").count()~ aggregates data based on unique values in a specified column.\n",
    "- **Sorting**: `Ddf.groupBy(\"column\").count().sort(\"column\", ascending=False)` sorts the aggregated results.\n",
    "- **Example**: Ranking businesses based on the number of stars and counting occurrences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Renaming Columns\n",
    "- **Renaming Columns**: `df.withColumnRenamed(\"old_name\", \"new_name\")` changes the name of a specified column.\n",
    "- **Usage**: Facilitates clarity and consistency in data frame schemas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Explode Function\n",
    "- **Purpose**: `explode` creates a separate row for each element in a list or array within a column.\n",
    "- **Application**: Useful for normalizing nested or complex data structures.\n",
    "- **Example**: Expanding a list of scores into individual rows for each score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conditional Statements with when\n",
    "- **Usage**: Implements if-else logic on a per-row basis.\n",
    "- **Syntax**: `f.when(condition, value).otherwise(other_value)` where `f` is an alias for `pyspark.sql.functions`.\n",
    "- **Example**: Creating a new column `Good` that assigns `1` `if` `score > 50`, `else` `0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Loading JSON Files and Understanding Schema\n",
    "- **Loading Data**: ``spark.read.json(\"file_path\")` loads JSON data into a data frame.\n",
    "- **Schema Examination**: `df.printSchema()` and `df.dtypes` help understand the structure and data types.\n",
    "- **Example Dataset**: Yelp Academic Dataset includes files like Business, Check-in, Review, TIP, and User."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Working with the Yelp Dataset\n",
    "- **Business Data Frame**:\n",
    "    - Contains fields such as address, attributes, business ID, categories, city, hours.\n",
    "    - Demonstrated methods to access and manipulate nested structures.\n",
    "- **Review, Check-in, TIP, and User Data Frames**:\n",
    "    - Loaded additional JSON files corresponding to different aspects of Yelp data.\n",
    "    - Explored schemas and performed operations like filtering and counting based on specific criteria.\n",
    "- **Example Operation**: Counting users with more than 5,000 cool compliments using `filter` and `count`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Items\n",
    "- **Practice Data Frame Operations**:\n",
    "    - `describe`, `show`, `select`, `filter`, `groupby`, `sort`, `rename`, and `explode` functions in PySpark.\n",
    "    - Create and drop columns using `withColumn` and `drop`.\n",
    "- **Load and Explore the Yelp Dataset**:\n",
    "    - Load JSON files for Business, Check-in, Review, TIP, and User.\n",
    "    - Examine schemas and perform basic data manipulations.\n",
    "- **Implement Conditional Logic**:\n",
    "    - Use the `when` function to create new columns based on specific conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow-up\n",
    "- **Upcoming Topics**:\n",
    "    - Detailed exploration of user-defined functions (UDFs) in PySpark.\n",
    "    - Advanced data manipulation techniques and optimizations.\n",
    "- **Next Weekâ€™s Focus**:\n",
    "    - Introduction to SQL's `SELECT` statement and its differences from PySpark's `select` function.\n",
    "- **Homework Assignment**:\n",
    "    - Tasks related to the Yelp dataset to reinforce data frame operations and schema understanding."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
