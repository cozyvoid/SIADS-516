{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This homework assignment builds on the Spark DataFrame material we covered in class.\n",
    "\n",
    "You will be using a compressed version of the Yelp Academic Dataset.  The data set is provided for you in the assets/data/yelp_academic of your workspace and you should not need to download it again if you're working on the Coursera hosted notebook environment.\n",
    "\n",
    "You might want to refer to the lecture companion notebooks (in resources/lecture_notebooks/ or equivalently via Coursera as \"Ungraded Lab: Spark Core Demo\" and \"Ungraded Lab: Spark SQL Demo) for hints about libraries to import, etc.\n",
    "\n",
    "You will notice that there are a **lot** of reviews.  You might want to work off a small sample (i.e. use the sample() function in Spark) to work on a reduced size dataset while you're developing your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The AutograderHelper class provides methods used by the autograder.\n",
    "from autograder_helper import AutograderHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autograder cell. This cell is worth 0 points.\n",
    "# This cell has hidden code used to configure the autograder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"local[*]\")\n",
    "    .appName(\"My First Spark application\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "sc = spark.sparkContext\n",
    "sc._conf.set(\"spark.default.parallelism\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some DataFrames:\n",
    "user = spark.read.json(\n",
    "    \"../../assets/data/yelp_academic/yelp_academic_dataset_user.json.gz\"\n",
    ")\n",
    "review = spark.read.json(\n",
    "    \"../../assets/data/yelp_academic/yelp_academic_dataset_review.json.gz\"\n",
    ")\n",
    "checkin = spark.read.json(\n",
    "    \"../../assets/data/yelp_academic/yelp_academic_dataset_checkin.json.gz\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 -- Cool Compliments\n",
    "\n",
    "Determine how many users have received more than 5000 \"cool\" compliments.\n",
    "\n",
    "- Create a variable `user_count` (an integer) which contains the number of user with more than 5000 \"cool\" compliments (using the `compliment_cool` field.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter users with more than 5000 \"cool\" compliments\n",
    "cool_users = user.filter(user.compliment_cool > 5000)\n",
    "\n",
    "# Count the number of such users\n",
    "user_count = cool_users.count()\n",
    "\n",
    "# Print the result\n",
    "print(f\"Number of users with more than 5000 'cool' compliments: {user_count}\")\n",
    "\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output\n",
    "```\n",
    "[Stage 3:>                                                          (0 + 1) / 1]\n",
    "Number of users with more than 5000 'cool' compliments: 79\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure user_count is an integer\n",
    "assert isinstance(user_count, int), \"The user_count variable should be an integer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autograder cell. This cell is worth 2 points (out of 20). This cell contains hidden tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's solve this by filtering the users who have received more than 5000 \"cool\" compliments and then counting them.\n",
    "\n",
    "### Steps:\n",
    "1. **Filter Users**: Filter the `user` DataFrame to include only users with more than 5000 \"cool\" compliments.\n",
    "2. **Count Users**: Count the number of users who meet this criterion.\n",
    "3. **Assign to Variable**: Assign the count to the variable `user_count`.\n",
    "\n",
    "### Explanation:\n",
    "- **Filter Users**: The `filter` method is used to select users with `compliment_cool` greater than 5000.\n",
    "- **Count Users**: The `count` method is used to get the number of users who meet the criterion.\n",
    "- **Assign to Variable**: The result is assigned to the variable `user_count`.\n",
    "\n",
    "Run this code to get the number of users with more than 5000 \"cool\" compliments. If you have any further questions or need additional assistance, feel free to ask! ðŸ˜ŠðŸ“Šâœ¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 -- Useful Positive Reviews\n",
    "\n",
    "Determine the top 5 most useful positive reviews.\n",
    "\n",
    "- Create a variable `top_5_useful_positive`. This should be a PySpark DataFrame\n",
    "- For this question a \"positive review\" is one with 4 or 5 stars\n",
    "- The DataFrame should be ordered by `useful` and contain 5 rows\n",
    "- The DataFrame should have these columns (in this order):\n",
    "    - `review_id`\n",
    "    - `useful`\n",
    "    - `stars`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filter positive reviews (4 or 5 stars)\n",
    "positive_reviews = review.filter((col(\"stars\") == 4) | (col(\"stars\") == 5))\n",
    "\n",
    "# Sort by \"useful\" in descending order\n",
    "sorted_positive_reviews = positive_reviews.orderBy(col(\"useful\").desc())\n",
    "\n",
    "# Select the top 5 reviews\n",
    "top_5_useful_positive = sorted_positive_reviews.select(\"review_id\", \"useful\", \"stars\").limit(5)\n",
    "\n",
    "# Show the result\n",
    "top_5_useful_positive.show()\n",
    "\n",
    "# Ensure the result is a DataFrame and has the correct columns\n",
    "assert type(top_5_useful_positive) == pyspark.sql.dataframe.DataFrame, \"The top_useful_positive variable should be a Spark DataFrame.\"\n",
    "assert top_5_useful_positive.columns == [\"review_id\", \"useful\", \"stars\"], \"The columns are not in the correct order.\"\n",
    "\n",
    "# Autograder cell\n",
    "submitted = AutograderHelper.parse_spark_dataframe(top_5_useful_positive)\n",
    "assert len(submitted) == 5, \"The result must have 5 rows.\"\n",
    "top_useful_review_id = \"1lGXlyq4MALOMx17vpBcoQ\"\n",
    "assert submitted[\"review_id\"][0] == top_useful_review_id, f'The first row should have review_id \"{top_useful_review_id}\" (this review has the most \"useful\" ratings)'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output\n",
    "```\n",
    "+--------------------+------+-----+\n",
    "|           review_id|useful|stars|\n",
    "+--------------------+------+-----+\n",
    "|1lGXlyq4MALOMx17v...|   358|  5.0|\n",
    "|gAUkgn4dTO-R2n5LB...|   278|  5.0|\n",
    "|5S985RjfmDJYsJvUt...|   244|  4.0|\n",
    "|0nr6SQFKpR6JCYl1z...|   241|  5.0|\n",
    "|-hRpmcavsC0UDI_Qs...|   235|  4.0|\n",
    "+--------------------+------+-----+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's solve this by filtering the reviews to include only positive reviews (4 or 5 stars), then sorting them by the \"useful\" column, and finally selecting the top 5.\n",
    "\n",
    "### Steps:\n",
    "1. **Filter Positive Reviews**: Filter the `review` DataFrame to include only reviews with 4 or 5 stars.\n",
    "2. **Sort by Useful**: Sort the filtered reviews by the \"useful\" column in descending order.\n",
    "3. **Select Top 5**: Select the top 5 reviews.\n",
    "4. **Select Required Columns**: Ensure the DataFrame has the columns `review_id`, `useful`, and `stars` in the correct order.\n",
    "\n",
    "### Explanation:\n",
    "- **Filter Positive Reviews**: The `filter` method is used to select reviews with 4 or 5 stars.\n",
    "- **Sort by Useful**: The `orderBy` method sorts the reviews by the \"useful\" column in descending order.\n",
    "- **Select Top 5**: The `limit` method selects the top 5 reviews.\n",
    "- **Select Required Columns**: The `select` method ensures the DataFrame has the columns `review_id`, `useful`, and `stars` in the correct order.\n",
    "\n",
    "Run this code to get the top 5 most useful positive reviews. If you have any further questions or need additional assistance, feel free to ask! ðŸ˜ŠðŸ“Šâœ¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 -- Checkins\n",
    "\n",
    "Determine what hours of the day most checkins occur.\n",
    "\n",
    "- Create a variable `hours_by_checkin_count`. This should be a PySpark DataFrame\n",
    "- The DataFrame should be ordered by `count` and contain 24 rows\n",
    "- The DataFrame should have these columns (in this order):\n",
    "    - `hour` (the hour of the day as an integer, the hour after midnight being `0`)\n",
    "    - `count` (the number of checkins that occurred in that hour)\n",
    "- Hour `1` (the time between 1:00 AM and 2:00 AM) should be the first entry in the results once they are sorted by the number of checkins that occurred in each hour.\n",
    "\n",
    "\n",
    "Note that the `date` column in the `checkin` data is a string with multiple date times in it. You'll need to split that string before parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, split, hour, col\n",
    "\n",
    "# Split the date column into individual timestamps and explode the array\n",
    "checkin_exploded = checkin.withColumn(\"timestamp\", explode(split(col(\"date\"), \",\")))\n",
    "\n",
    "# Extract the hour from the timestamp\n",
    "checkin_with_hour = checkin_exploded.withColumn(\"hour\", hour(col(\"timestamp\")))\n",
    "\n",
    "# Count the number of check-ins for each hour\n",
    "hourly_checkin_counts = checkin_with_hour.groupBy(\"hour\").count()\n",
    "\n",
    "# Sort the results by count in descending order\n",
    "sorted_hourly_checkin_counts = hourly_checkin_counts.orderBy(col(\"count\").desc())\n",
    "\n",
    "# Select the required columns\n",
    "hours_by_checkin_count = sorted_hourly_checkin_counts.select(\"hour\", \"count\")\n",
    "\n",
    "# Show the result\n",
    "hours_by_checkin_count.show(24)\n",
    "\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output\n",
    "```\n",
    "+----+-------+\n",
    "|hour|  count|\n",
    "+----+-------+\n",
    "|   1|1561788|\n",
    "|  19|1502271|\n",
    "|   0|1491176|\n",
    "|   2|1411255|\n",
    "|  20|1350195|\n",
    "|  23|1344117|\n",
    "|  18|1272108|\n",
    "|  22|1257437|\n",
    "|  21|1238808|\n",
    "|   3|1078939|\n",
    "|  17|1006102|\n",
    "|  16| 852076|\n",
    "|   4| 747453|\n",
    "|  15| 617830|\n",
    "|   5| 485129|\n",
    "|  14| 418340|\n",
    "|   6| 321764|\n",
    "|  13| 270145|\n",
    "|   7| 231417|\n",
    "|  12| 178910|\n",
    "|   8| 151065|\n",
    "|  11| 111769|\n",
    "|   9| 100568|\n",
    "|  10|  88486|\n",
    "+----+-------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the result is a DataFrame and has the correct columns\n",
    "assert (\n",
    "    type(hours_by_checkin_count) == pyspark.sql.dataframe.DataFrame\n",
    "), \"The hours_by_checkin_count variable should be a Spark DataFrame.\"\n",
    "\n",
    "assert hours_by_checkin_count.columns == [\n",
    "    \"hour\",\n",
    "    \"count\",\n",
    "], \"The columns are not in the correct order.\"\n",
    "\n",
    "submitted = AutograderHelper.parse_spark_dataframe(hours_by_checkin_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autograder cell. This cell is worth 1 point (out of 20). This cell does not contain hidden tests.\n",
    "\n",
    "assert len(submitted) == 24, \"The hours_by_checkin_count DataFrame must have 24 rows.\"\n",
    "\n",
    "assert submitted[\"hour\"][0] == 1, \"The first row should have hour 1\"\n",
    "assert submitted[\"hour\"][1] == 19, \"The second row should have hour 19\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autograder cell. This cell is worth 4 points (out of 20). This cell contains hidden tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's solve this by extracting the hour from the check-in timestamps, counting the number of check-ins for each hour, and then sorting the results.\n",
    "\n",
    "### Steps:\n",
    "1. **Extract Hours**: Extract the hour from the check-in timestamps.\n",
    "2. **Count Check-ins by Hour**: Count the number of check-ins for each hour.\n",
    "3. **Sort by Count**: Sort the results by the count of check-ins in descending order.\n",
    "4. **Select Required Columns**: Ensure the DataFrame has the columns `hour` and `count` in the correct order.\n",
    "\n",
    "Here's the code to achieve this:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import explode, split, hour, col\n",
    "\n",
    "# Split the date column into individual timestamps and explode the array\n",
    "checkin_exploded = checkin.withColumn(\"timestamp\", explode(split(col(\"date\"), \",\")))\n",
    "\n",
    "# Extract the hour from the timestamp\n",
    "checkin_with_hour = checkin_exploded.withColumn(\"hour\", hour(col(\"timestamp\")))\n",
    "\n",
    "# Count the number of check-ins for each hour\n",
    "hourly_checkin_counts = checkin_with_hour.groupBy(\"hour\").count()\n",
    "\n",
    "# Sort the results by count in descending order\n",
    "sorted_hourly_checkin_counts = hourly_checkin_counts.orderBy(col(\"count\").desc())\n",
    "\n",
    "# Select the required columns\n",
    "hours_by_checkin_count = sorted_hourly_checkin_counts.select(\"hour\", \"count\")\n",
    "\n",
    "# Show the result\n",
    "hours_by_checkin_count.show(24)\n",
    "\n",
    "# Ensure the result is a DataFrame and has the correct columns\n",
    "assert type(hours_by_checkin_count) == pyspark.sql.dataframe.DataFrame, \"The hours_by_checkin_count variable should be a Spark DataFrame.\"\n",
    "assert hours_by_checkin_count.columns == [\"hour\", \"count\"], \"The columns are not in the correct order.\"\n",
    "\n",
    "# Autograder cell\n",
    "submitted = AutograderHelper.parse_spark_dataframe(hours_by_checkin_count)\n",
    "assert len(submitted) == 24, \"The hours_by_checkin_count DataFrame must have 24 rows.\"\n",
    "assert submitted[\"hour\"][0] == 1, \"The first row should have hour 1\"\n",
    "assert submitted[\"hour\"][1] == 19, \"The second row should have hour 19\"\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "- **Extract Hours**: The `explode` and `split` functions are used to split the date column into individual timestamps, and the `hour` function extracts the hour from each timestamp.\n",
    "- **Count Check-ins by Hour**: The `groupBy` and `count` methods are used to count the number of check-ins for each hour.\n",
    "- **Sort by Count**: The `orderBy` method sorts the results by the count of check-ins in descending order.\n",
    "- **Select Required Columns**: The `select` method ensures the DataFrame has the columns `hour` and `count` in the correct order.\n",
    "\n",
    "Run this code to get the distribution of check-ins by hour. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 -- Common Words in Useful Reviews\n",
    "\n",
    "Write a function that takes a Spark DataFrame as a parameter and returns a Spark DataFrame of the 50 most common words from *useful* reviews and their counts.\n",
    "\n",
    "- A \"useful review\" has 10 or more \"useful\" ratings.\n",
    "- Convert the text to lower case.\n",
    "- Use the provided `splitter()` function in a UDF to split the text into individual words.\n",
    "- Exclude the words in the provided `STOP_WORDS` set.\n",
    "- Returned DataFrame should have these columns (in this order):\n",
    "    - `word`\n",
    "    - `count`\n",
    "- Returned DataFrame should be sorted by `count` in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.0' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/saman/AppData/Local/Programs/Python/Python313-32/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import re \n",
    "def splitter(text): WORD_RE = re.compile(r\"[\\w']+\") \n",
    "return WORD_RE.findall(text) \n",
    "\n",
    "STOP_WORDS = { \n",
    "    \"a\", \n",
    "    \"about\", \n",
    "    \"above\", \n",
    "    \"after\", \n",
    "    \"again\", \n",
    "    \"against\", \n",
    "    \"aint\", \n",
    "    \"all\", \n",
    "    \"also\", \n",
    "    \"although\", \n",
    "    \"am\", \n",
    "    \"an\", \n",
    "    \"and\", \n",
    "    \"any\", \n",
    "    \"are\", \n",
    "    \"as\", \n",
    "    \"at\", \n",
    "    \"be\", \n",
    "    \"because\", \n",
    "    \"been\", \n",
    "    \"before\", \n",
    "    \"being\", \n",
    "    \"below\", \n",
    "    \"between\", \n",
    "    \"both\", \n",
    "    \"but\", \n",
    "    \"by\", \n",
    "    \"can\", \n",
    "    \"check\", \n",
    "    \"checked\", \n",
    "    \"could\", \n",
    "    \"did\", \n",
    "    \"do\", \n",
    "    \"does\", \n",
    "    \"doing\", \n",
    "    \"don\", \n",
    "    \"down\", \n",
    "    \"during\", \n",
    "    \"each\", \n",
    "    \"few\", \n",
    "    \"for\", \n",
    "    \"from\", \n",
    "    \"further\", \n",
    "    \"get\", \n",
    "    \"go\", \n",
    "    \"got\", \n",
    "    \"had\", \n",
    "    \"has\", \n",
    "    \"have\", \n",
    "    \"having\", \"he\", \"her\", \"here\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \n",
    "    \"how\", \"however\", \"i\", \"i'd\", \"if\", \"i'm\", \"in\", \"into\", \"is\", \"it\", \"its\", \n",
    "    \"it's\", \"itself\", \"i've\", \n",
    "    \"just\", \n",
    "    \"me\", \n",
    "    \"more\", \n",
    "    \"most\", \n",
    "    \"my\", \n",
    "    \"myself\", \n",
    "    \"no\", \n",
    "    \"nor\", \n",
    "    \"not\", \n",
    "    \"now\", \n",
    "    \"of\", \n",
    "    \"off\", \n",
    "    \"on\", \n",
    "    \"once\", \n",
    "    \"one\", \n",
    "    \"online\", \n",
    "    \"only\", \n",
    "    \"or\", \n",
    "    \"other\", \n",
    "    \"our\", \n",
    "    \"ours\", \n",
    "    \"ourselves\", \n",
    "    \"out\", \n",
    "    \"over\", \n",
    "    \"own\", \n",
    "    \"paid\", \n",
    "    \"place\", \n",
    "    \"s\", \n",
    "    \"said\", \n",
    "    \"same\", \n",
    "    \"service\", \n",
    "    \"she\", \n",
    "    \"should\", \n",
    "    \"so\", \n",
    "    \"some\", \n",
    "    \"such\", \n",
    "    \"t\", \n",
    "    \"than\", \n",
    "    \"that\", \n",
    "    \"the\", \n",
    "    \"their\", \n",
    "    \"theirs\", \n",
    "    \"them\", \n",
    "    \"themselves\", \n",
    "    \"then\", \n",
    "    \"there\", \n",
    "    \"these\", \n",
    "    \"they\", \n",
    "    \"this\", \n",
    "    \"those\", \n",
    "    \"through\", \n",
    "    \"to\", \n",
    "    \"too\", \n",
    "    \"under\", \n",
    "    \"until\", \n",
    "    \"up\", \n",
    "    \"us\", \n",
    "    \"very\", \n",
    "    \"was\", \n",
    "    \"we\", \n",
    "    \"went\", \n",
    "    \"were\", \n",
    "    \"we've\", \n",
    "    \"what\", \n",
    "    \"when\", \n",
    "    \"where\", \n",
    "    \"which\", \n",
    "    \"while\", \n",
    "    \"who\", \n",
    "    \"whom\", \n",
    "    \"why\", \n",
    "    \"will\", \n",
    "    \"with\", \n",
    "    \"would\", \n",
    "    \"you\", \n",
    "    \"your\", \n",
    "    \"yours\", \n",
    "    \"yourself\", \n",
    "    \"yourselves\", \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lower, udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "# Define the splitter UDF\n",
    "splitter_udf = udf(splitter, ArrayType(StringType()))\n",
    "\n",
    "def common_useful_words(reviews, limit=50):\n",
    "    # Filter useful reviews (10 or more useful ratings)\n",
    "    useful_reviews = reviews.filter(col(\"useful\") >= 10)\n",
    "    \n",
    "    # Convert text to lower case and split into words\n",
    "    words = useful_reviews.withColumn(\"words\", splitter_udf(lower(col(\"text\"))))\n",
    "    \n",
    "    # Explode the words into individual rows\n",
    "    exploded_words = words.selectExpr(\"explode(words) as word\")\n",
    "    \n",
    "    # Exclude stop words\n",
    "    filtered_words = exploded_words.filter(~col(\"word\").isin(STOP_WORDS))\n",
    "    \n",
    "    # Count the occurrences of each word\n",
    "    word_counts = filtered_words.groupBy(\"word\").count()\n",
    "    \n",
    "    # Sort by count in descending order\n",
    "    sorted_word_counts = word_counts.orderBy(col(\"count\").desc())\n",
    "    \n",
    "    # Select the top 50 most common words\n",
    "    most_common = sorted_word_counts.limit(limit)\n",
    "    \n",
    "    return most_common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output\n",
    "```\n",
    "+----------+------+\n",
    "|      word| count|\n",
    "+----------+------+\n",
    "|      like|101251|\n",
    "|      time| 86124|\n",
    "|      good| 83486|\n",
    "|      back| 71308|\n",
    "|      food| 65281|\n",
    "|      even| 58499|\n",
    "|    really| 57687|\n",
    "|     don't| 56146|\n",
    "|     great| 55402|\n",
    "|      well| 48297|\n",
    "|    didn't| 45751|\n",
    "|     first| 43738|\n",
    "|    people| 42768|\n",
    "|      know| 40954|\n",
    "|     never| 40741|\n",
    "|         2| 39573|\n",
    "|      told| 39350|\n",
    "|       day| 38164|\n",
    "|      came| 38098|\n",
    "|      much| 37227|\n",
    "|         5| 37005|\n",
    "|       two| 36824|\n",
    "|      make| 36542|\n",
    "|       see| 36153|\n",
    "|      made| 35590|\n",
    "|       way| 35177|\n",
    "|      nice| 35041|\n",
    "|      come| 34448|\n",
    "|     going| 33635|\n",
    "|      take| 33453|\n",
    "|       new| 33279|\n",
    "|         3| 33193|\n",
    "|    little| 32951|\n",
    "|      want| 32752|\n",
    "|     order| 32097|\n",
    "|     right| 31696|\n",
    "|experience| 31680|\n",
    "|     still| 31566|\n",
    "|       try| 30273|\n",
    "|     since| 30141|\n",
    "|    around| 29654|\n",
    "|   another| 29154|\n",
    "|      room| 28892|\n",
    "|      best| 28462|\n",
    "|       say| 28215|\n",
    "|     asked| 28180|\n",
    "|      menu| 27998|\n",
    "|     vegas| 27351|\n",
    "|restaurant| 27341|\n",
    "|     think| 27321|\n",
    "+----------+------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll run it on the `review` DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_useful_words_counts = common_useful_words(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    type(common_useful_words_counts) == pyspark.sql.dataframe.DataFrame\n",
    "), \"The common_useful_words_counts variable should be a Spark DataFrame.\"\n",
    "\n",
    "assert common_useful_words_counts.columns == [\n",
    "    \"word\",\n",
    "    \"count\",\n",
    "], \"The columns are not in the correct order.\"\n",
    "\n",
    "submitted = AutograderHelper.parse_spark_dataframe(common_useful_words_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autograder cell. This cell is worth 2 points (out of 20). This cell does not contain hidden tests.\n",
    "\n",
    "assert (\n",
    "    len(submitted) == 50\n",
    "), \"The common_useful_words_counts DataFrame must have 50 rows.\"\n",
    "\n",
    "assert submitted[\"word\"][0] == \"like\", 'The first row should have word \"like\"'\n",
    "\n",
    "assert submitted[\"count\"][0] == 101251, \"The first row should have count 101251\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autograder cell. This cell is worth 6 points (out of 20). This cell contains hidden tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's solve this by filtering the reviews to include only useful reviews (10 or more useful ratings), splitting the text into individual words, excluding stop words, and then counting the most common words.\n",
    "\n",
    "### Steps:\n",
    "1. **Filter Useful Reviews**: Filter the `review` DataFrame to include only reviews with 10 or more useful ratings.\n",
    "2. **Convert Text to Lower Case**: Convert the review text to lower case.\n",
    "3. **Split Text into Words**: Use the provided `splitter` function to split the text into individual words.\n",
    "4. **Exclude Stop Words**: Exclude the words in the provided `STOP_WORDS` set.\n",
    "5. **Count Words**: Count the occurrences of each word.\n",
    "6. **Sort by Count**: Sort the results by count in descending order.\n",
    "7. **Select Top 50**: Select the top 50 most common words.\n",
    "\n",
    "Here's the code to achieve this:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import col, lower, udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "# Define the splitter UDF\n",
    "splitter_udf = udf(splitter, ArrayType(StringType()))\n",
    "\n",
    "def common_useful_words(reviews, limit=50):\n",
    "    # Filter useful reviews (10 or more useful ratings)\n",
    "    useful_reviews = reviews.filter(col(\"useful\") >= 10)\n",
    "    \n",
    "    # Convert text to lower case and split into words\n",
    "    words = useful_reviews.withColumn(\"words\", splitter_udf(lower(col(\"text\"))))\n",
    "    \n",
    "    # Explode the words into individual rows\n",
    "    exploded_words = words.selectExpr(\"explode(words) as word\")\n",
    "    \n",
    "    # Exclude stop words\n",
    "    filtered_words = exploded_words.filter(~col(\"word\").isin(STOP_WORDS))\n",
    "    \n",
    "    # Count the occurrences of each word\n",
    "    word_counts = filtered_words.groupBy(\"word\").count()\n",
    "    \n",
    "    # Sort by count in descending order\n",
    "    sorted_word_counts = word_counts.orderBy(col(\"count\").desc())\n",
    "    \n",
    "    # Select the top 50 most common words\n",
    "    most_common = sorted_word_counts.limit(limit)\n",
    "    \n",
    "    return most_common\n",
    "\n",
    "# Run the function on the review DataFrame\n",
    "common_useful_words_counts = common_useful_words(review)\n",
    "\n",
    "# Show the result\n",
    "common_useful_words_counts.show(50)\n",
    "\n",
    "# Ensure the result is a DataFrame and has the correct columns\n",
    "assert type(common_useful_words_counts) == pyspark.sql.dataframe.DataFrame, \"The common_useful_words_counts variable should be a Spark DataFrame.\"\n",
    "assert common_useful_words_counts.columns == [\"word\", \"count\"], \"The columns are not in the correct order.\"\n",
    "\n",
    "# Autograder cell\n",
    "submitted = AutograderHelper.parse_spark_dataframe(common_useful_words_counts)\n",
    "assert len(submitted) == 50, \"The common_useful_words_counts DataFrame must have 50 rows.\"\n",
    "assert submitted[\"word\"][0] == \"like\", 'The first row should have word \"like\"'\n",
    "assert submitted[\"count\"][0] == 101251, \"The first row should have count 101251\"\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "- **Filter Useful Reviews**: The `filter` method is used to select reviews with 10 or more useful ratings.\n",
    "- **Convert Text to Lower Case**: The `lower` function converts the review text to lower case.\n",
    "- **Split Text into Words**: The `splitter` UDF splits the text into individual words.\n",
    "- **Exclude Stop Words**: The `filter` method excludes words in the `STOP_WORDS` set.\n",
    "- **Count Words**: The `groupBy` and `count` methods count the occurrences of each word.\n",
    "- **Sort by Count**: The `orderBy` method sorts the results by count in descending order.\n",
    "- **Select Top 50**: The `limit` method selects the top 50 most common words.\n",
    "\n",
    "Run this code to get the 50 most common words from useful reviews. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
