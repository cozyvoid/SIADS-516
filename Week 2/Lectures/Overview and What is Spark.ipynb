{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overview**\n",
    "This material covers an introduction to `Apache Spark` and `Resilient Distributed Datasets` (`RDDs`) as part of a course focusing on distributed computing. The aim is to provide an understanding of Spark's capabilities, components, and operations within the context of data processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction to Spark**\n",
    "- **Definition**: Spark is described as a cluster computing platform that extends MapReduce with features for interactive queries, exploratory data analysis, and stream processing.\n",
    "- **Capabilities**: Spark supports complex workflows integrating various types of analyses like data manipulation and machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spark Stack**\n",
    "- **Focus on Spark Core**: `Spark Core` is responsible for basic functionalities like task scheduling, memory management, storage interaction, and fault recovery. It interacts effectively with `Hadoop`, particularly with `HDFS` (`Hadoop Distributed File System`), although it doesn’t require Hadoop.\n",
    "- **Integration**: Spark can utilize different storage formats and work seamlessly with Hadoop components like `Yarn` for resource management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resilient Distributed Datasets (RDDs)**\n",
    "- **Operations**: You can create, transform, and perform actions on RDDs. This forms the core of data manipulation in Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Components**\n",
    "- **SparkSession and SparkContext**:\n",
    "    - **SparkSession**: Acts as an entry point for a Spark application, managing distributed computing resources.\n",
    "    - **SparkContext**: A part of SparkSession that handles basic operations. It is established through boilerplate code provided in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SparkSession Setup**\n",
    "- **Code Explanation**: A breakdown is provided on creating a SparkSession using `PySpark`, including the use of `SparkSession.builder`, setting of the master node to utilize available local processors, and defining an optional app name for distinguishing instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Architecture**\n",
    "- **Driver and Executors**:\n",
    "    - **Driver**: Part of the Spark Session, responsible for distributing workloads and managing component execution across a computational cluster.\n",
    "    - **Executors**: Nodes or processors that carry out the tasks distributed by the driver, abstracting infrastructure details from the user to allow ease of use for data scientists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Takeaways**\n",
    "- Spark offers a powerful environment for processing big data through its RDD model and parallel computing capabilities.\n",
    "- The proper setup of SparkSessions and understanding of SparkContext are critical for leveraging Spark’s functionalities effectively.\n",
    "- Spark abstracts much of the hardware interaction, allowing data scientists to focus more on data processing tasks rather than technical infrastructure details."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
