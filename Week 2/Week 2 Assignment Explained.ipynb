{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Part-of-Speech (POS) Counts\n",
    "**Objective**: Create an RDD pipeline to count the occurrences of each part-of-speech tag in a text dataset.\n",
    "\n",
    "## Steps:\n",
    "## 1. Read the Text File:\n",
    "- Load the text data using `sc.textFile` to create an RDD from the dataset.\n",
    "- Example: `text = sc.textFile(\"path/to/textfile.txt\")`\n",
    "## 2. Filter the Data:\n",
    "- Filter out blank lines and lines starting with 'URL'.\n",
    "## 3. Tokenization and POS Tagging:\n",
    "- Define a function `pos_tag_counter` that tokenizes each line and tags each token with its part-of-speech.\n",
    "- Use `nltk` for tokenization and POS tagging.\n",
    "- Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag_counter(line):\n",
    "    tokens = nltk.word_tokenize(line)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    return pos_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. FlatMap to Apply POS Tagging:\n",
    "- Use `flatMap` to apply `pos_tag_counter` to each entry in the RDD.\n",
    "- Example: `pos_tagged_rdd = filtered_rdd.flatMap(pos_tag_counter)`\n",
    "## 5. Map and Reduce by Key:\n",
    "- Map each POS tag to a key-value pair where the key is the tag and the value is 1.\n",
    "- Reduce by key to get the count of each POS tag.\n",
    "- Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_counts_rdd = pos_tagged_rdd.map(lambda x: (x[1], 1)).reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sort Results:\n",
    "- Sort the results by counts in descending order.\n",
    "- Example: `sorted_pos_counts_rdd = pos_counts_rdd.sortBy(lambda x: x[1], ascending=False)`\n",
    "## 7. Output the Results:\n",
    "- Take the top 10 results and print them.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_pos_counts = sorted_pos_counts_rdd.take(10)\n",
    "for pos, count in top_10_pos_counts:\n",
    "    print(f\"{pos}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Noun Phrase Length Distribution\n",
    "**Objective**: Create an RDD pipeline to show the distribution of the length of noun phrases in a text dataset.\n",
    "## Steps:\n",
    "## 1. Define the Grammar for Noun Phrases:\n",
    "- Define a grammar to identify noun phrases using `nltk.RegexpParser`.\n",
    "- Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "    NBAR:\n",
    "        {<NN.*|JJS>*<NN.*>}\n",
    "    NP:\n",
    "        {<NBAR>}\n",
    "        {<NBAR><IN><NBAR>}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define a Function to Extract Noun Phrases:\n",
    "- Create a function `get_noun_phrases` that tokenizes and POS tags each line, then extracts noun phrases based on the defined grammar.\n",
    "- Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noun_phrases(line):\n",
    "    tokens = nltk.word_tokenize(line)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    chunker = nltk.RegexpParser(grammar)\n",
    "    tree = chunker.parse(pos_tags)\n",
    "    return [subtree.leaves() for subtree in tree.subtrees(filter=lambda t: t.label() == 'NP')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. FlatMap to Apply Noun Phrase Extraction:\n",
    "- Use `flatMap` to apply `get_noun_phrases` to each entry in the RDD.\n",
    "- Example: `noun_phrases_rdd = text.flatMap(get_noun_phrases)`\n",
    "## 4. Map and Reduce by Key:\n",
    "- Map each noun phrase to a key-value pair where the key is the length of the noun phrase and the value is 1.\n",
    "- Reduce by key to get the count of each length.\n",
    "- Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_rdd = noun_phrases_rdd.map(lambda np: (len(np), 1)).reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sort Results:\n",
    "- Sort the results by counts in descending order.\n",
    "- Example: `sorted_length_counts_rdd = length_rdd.sortBy(lambda x: x[1], ascending=False)`\n",
    "## 6. Output the Results:\n",
    "- Take the top 10 results and print them.\n",
    "- Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_length_counts = sorted_length_counts_rdd.take(10)\n",
    "for length, count in top_10_length_counts:\n",
    "    print(f\"Length: {length}, Count: {count}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
