{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resilient Distributed Data Sets (RDDs) in Spark**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction to RDDs**\n",
    "- RDDs are the fundamental abstraction for distributed data computation in Spark.\n",
    "- They are immutable collections of objects that can be distributed across a cluster.\n",
    "- RDDs support parallel operations and are suitable for unstructured data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Operations on RDDs**\n",
    "- **Creation**: RDDs can be created by reading files or by parallelizing collections like lists or sets.\n",
    "    - Using `SparkContext.textFile` reads a text file as an RDD, with each line as an individual element.\n",
    "    - `SparkContext.parallelize` can take a collection and create an RDD by distributing its elements across a cluster.\n",
    "- **Transformations**: These create new RDDs from existing ones, such as filtering, mapping, or reducing.\n",
    "- **Actions**: Actions trigger execution and return non-RDD results, like counts or lists.\n",
    "    - Common actions include `count` (returns the number of elements), `collect` (gathers all elements into a list), `take` (retrieves the first N elements), and `first` (retrieves the first element)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Detailed Explanation of Transformations and Actions**\n",
    "- **RDD Transformations**: These are lazy operations that transform one RDD into another.\n",
    "    - Example: Filtering RDDs to count lines containing a specific word using a lambda function or a defined function.\n",
    "- **RDD Actions**: Execute transformations and return results to the driver program.\n",
    "    - Be cautious using `collect` on large data sets as it might overload the memory of the driver.\n",
    "- SPARK uses a Directed Acyclic Graph (DAG) for computations, leveraging lazy evaluation for efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Partitioning**\n",
    "- Data is partitioned in multiples of 64 MB by default, which can be tuned based on the number of cores available.\n",
    "- Proper partitioning ensures efficient distribution across cluster nodes. Local file system reads aren't partitioned, but parallelized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practical Examples**\n",
    "1. **Filtering**:\n",
    "- A filter transformation can be used to select elements that meet a specific criteria, such as lines containing a specific word.\n",
    "2. **Mapping**:\n",
    "- `map` transformation applies a function to each element of the RDD, creating a new RDD.\n",
    "- Example: Squaring elements of an RDD of numbers.\n",
    "3. **Flat Mapping**:\n",
    "- `flatMap` is similar to `map`, but it ensures results are not nested, returning a flat structure instead.\n",
    "- Useful in text processing for splitting lines into words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "- RDDs are essential for handling distributed data in Spark, allowing for fault-tolerant operations on large datasets.\n",
    "- Understanding basic operations like creation, actions, and transformations is key to leveraging Spark for big data processing.\n",
    "- Use transformations wisely to project data into desired forms without materializing intermediate datasets, ensuring efficiency."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
